{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vicky Pollard and English Word Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Victoria 'Vicky' Pollard is a popular character from the TV and radio series *Little Britain*. She is a moody, obnoxious teenage girl seemingly incapable of doing much but gossip in a strong Bristol accent. She is a representation of teenagers and chavs.\n",
    "\n",
    "To get started on this assignment, read the BBC News article: UK's Vicky Pollard's \n",
    "[left behind.](http://news.bbc.co.uk/1/hi/education/6173441.stm)\n",
    "The article gives the following statistic about teen language: \n",
    "\"The top 20 words used, including *yeah, no, but* and *like*, account \n",
    "for around a third of all words.\"  The idea is that this shows\n",
    "that teenagers speak an impoverished variety of English.  Much\n",
    "of their speaking time is used up uttering one of a very small\n",
    "set of words.\n",
    "\n",
    "But is that really a **significant** fact about teen speech?\n",
    "Let's look at a (relatively) random sample of English,\n",
    "count up the 20 most frequent words, and answer the following question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:  What proportion of all word tokens are covered by those top 20 words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1 is the question you must answer to complete this\n",
    "exercise.\n",
    "\n",
    "To answer this question, you will use a **balanced corpus**\n",
    "of English texts, a corpus collected with the purpose of representing\n",
    "a balanced variety of English text types: fiction, poetry, speech,\n",
    "non fiction, and so on.  One relatively well-established, free,\n",
    "and easy-to-get example of such a corpus is the **Brown Corpus.**\n",
    "Brown is about 1.2 M words. \n",
    "\n",
    "To get the Brown corpus, do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/gawron/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following expression returns a list of all 1.2 M word tokens in Brow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " brown.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through this list and make a frequency distribution dictionary\n",
    "(the value associated with word `w` is the count of how many times \n",
    "`w` has occurred in Brown). Then do some more computing to abswer Question 1.\n",
    "What proportion of the total words is taken up by the top\n",
    "20 words?\n",
    "\n",
    "Here's Question 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:  What do you conclude about the statistic cited in the Vicky Pollard article? Does the statistic cited demonstrate that teenagers speak with an impoverished vocabulary?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about this on \n",
    "[LanguageLog.](http://itre.cis.upenn.edu/~myl/languagelog/archives/003993.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful hints and code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw0 = brown.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name `bw` has now been set to the list of all the word tokens in Brown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bw0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a data structure with all the word counts, do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd0 = nltk.FreqDist(bw0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done all that you can now execute the following to find the top n most frequent words and their counts.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 62713), (',', 58334), ('.', 49346)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd0.most_common(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What `fd` is is an`nltk FreqDist` (Frequency Distribution), essentially, a Python dictionary containing the count of all the word types in Brown (it also has some features customized for word frequencies, like the `most_common` method).  To find the number of word tokens in Brown, find the length of `bw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bw0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, about 1.16 Million words.  You can also use another one of the customized methods included with a `FreqDist`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd0.N()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out the number of **word types** in Brown, just get the length of the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56057"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fd0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I said `fd` was a dictionary of word counts and it is.  Here's how to use it as a Python dictionary:\n",
    "To get the count of the word type ‘computer’ do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd0['computer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means there are 13 *tokens* of the word type 'computer' in the Brown corpus.  Note that case matters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd0['Computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62713, 7258)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd0['the'],fd0['The']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is handy to ignore case for purposes of this exercise. To do that, just lowercase all the words in Brown before making the frequency distribution. This changes the number of word types, but not the number of word tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1161192, 1161192)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This uses what Pythonistas call a list comprehension.\n",
    "# It's just a compact way of writing a loop and collecting results in  a list.\n",
    "bw1 = [w.lower() for w in bw0]\n",
    "len(bw1),len(bw0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the **corpus** (the body of data) hasn't changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56057, 49815)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd1 = nltk.FreqDist(bw1)\n",
    "len(fd0),len(fd1)\n",
    "#49815"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary has shrunk because we now collapsed the words 'The' and 'the'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69971"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd1['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd1['The']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find it useful to take a look at [Chapter One of the NLTK book.](http://www.nltk.org/book/ch01.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To hand in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. An answers to questions 1 and 2.\n",
    "\n",
    "2. The code you used to answer question 1.  The simplest way to do that is to hand in an edtited version of this Python notebook.  When you want to type in an answer to a question, create a notebook cell by hovering your cursor over the bottom center of a cell and selecting \"+ Text\".  Then type your answer in the cell that appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20 = sum(ct for (wd, ct) in fd1.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd1.N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3598250763009046"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_20/fd1.N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw2 = [w.lower() for w in brown.words() if w not in ',!?.;()``\\\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1027919"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd2 = nltk.FreqDist(bw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.274169990837002\n"
     ]
    }
   ],
   "source": [
    "new_top_20 = sum(ct for wd, ct in fd2.most_common(20))\n",
    "print(new_top_20/fd.N())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The top 20 words used, including yeah, no, but and like, account for around a third of all words.\"\n",
    "\n",
    "Well in the first way of counting, which counted punctuation as words, the top 20\n",
    "words accounted for one-third of all tokens.  So the Pollard teenagers aren't all\n",
    "that different from a balanced English corpus, so **this statistic** at least does not\n",
    "support the claim that they have an impoverished vocabulary.\n",
    "\n",
    "In the second wy of counting, the top 20 words for Pollard teenagers take up a \n",
    "somewhat greater proportion of the words than the top 20 words\n",
    "in Brown, so the claim has a little more plausibility, but not much.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
