{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Pollard assignment you computed a unigram frequency distribution for the Brown corpus. You will need that for this assignmewnt.\n",
    "\n",
    "This time you will do a bigram distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk import bigrams\n",
    "brown_bigrams = list(bigrams(brown.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is instructive to compare brown.words, which we used in the last assignment, with brown.bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.words[:10]\n",
    "#['The', 'Fulton', 'County', 'Grand', 'Jury', 'said',\n",
    "# 'Friday', 'an', 'investigation', 'of']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_bigrams[:10]\n",
    "#[('The', 'Fulton'), ('Fulton', 'County'), ('County', 'Grand'),\n",
    "#('Grand', 'Jury'), ('Jury', 'said'), ('said', 'Friday'), ('Friday', 'an'),\n",
    "#('an', 'investigation'), ('investigation', 'of'), ('of', \"Atlanta's\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So brown.words() returns a list of the words, while brown.bigrams() returns a list of word pairs. Notice the the second word of the first pair becomes the first word of the second pair, and the the second word of the second pair, the first word of the third, and so on. Since each word in Brown becaome the first word of a bigram except the last, there is exactly one more word token than there are bogram tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(brown_bigrams)\n",
    "#1161191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(brown.words())\n",
    "1161192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new frequency distribution of the Brown bigrams. Plot the cumulative frequency distribution of the top 50 bigrams.\n",
    "\n",
    "Then do add one smoothing on the bigrams. This will require adding one to all the bigram counts, including those that previously had count 0. You will also need to change the ungram counts appropriately. You will compute all possible bigrams using the known vocabulary, so use the keys of the unigram Brown distribution you created before to compute the set of possible bigrams. The vocabulary size from that exercise should be 49815. Then having added 1 to all the bigram counts, you must compute at least the following Probabilities:\n",
    "\n",
    "\n",
    "1. P(the | in) before and after smoothing (P_{\\text{mle}} and P_{\\text{laplace}});\n",
    "\n",
    "2.  P(in the) before and after smoothing;\n",
    "\n",
    "3.  P(said the) before and after smoothing.\n",
    "\n",
    "4. P(the | said) before and after smoothing.\n",
    "\n",
    "In some cases you will to use the unigram counts to compute these probabilities. Remember that the unigram counts must change too when smoothing.\n",
    "\n",
    "Turn in these values and the Python code you used to compute them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "wds = brown.words()\n",
    "N = len(wds)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161192\n",
      "1161191\n"
     ]
    }
   ],
   "source": [
    "mle_unigram_dist = nltk.FreqDist([w.lower() for w in wds])\n",
    "bigram_seq = list(nltk.bigrams(wds))\n",
    "bigram_N = len(bigram_seq)\n",
    "print(bigram_N)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bigram_N` = `N - 1`.  Here's why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'Fulton'),\n",
       " ('Fulton', 'County'),\n",
       " ('County', 'Grand'),\n",
       " ('Grand', 'Jury'),\n",
       " ('Jury', 'said'),\n",
       " ('said', 'Friday'),\n",
       " ('Friday', 'an'),\n",
       " ('an', 'investigation'),\n",
       " ('investigation', 'of'),\n",
       " ('of', \"Atlanta's\")]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_seq[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first bigram starts with the first word, the second with second word and so on.  But there is no bigram\n",
    "that starts with the last word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a frequency distribution for bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE stands for Maximum Likelihood Estimate\n",
    "mle_bigram_dist = nltk.FreqDist((x.lower(),y.lower()) for (x,y) in bigram_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 49815 samples and 1161192 outcomes>\n",
      "69971\n",
      "<FreqDist with 436003 samples and 1161191 outcomes>\n",
      "258\n"
     ]
    }
   ],
   "source": [
    "print(mle_unigram_dist)\n",
    "print(mle_unigram_dist['the'])\n",
    "print(mle_bigram_dist)\n",
    "print(mle_bigram_dist['the','only'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information printed about `mle_unigram_dist`: The vocabulary has 49,815 word types.  The Brown corpus has 1,161,192 word tokens.\n",
    "\n",
    "The information printed about `mle_bigram_dist`: The \"vocabulary\" (of bigrams) has 436,003 bigram types.  The Brown corpus has 1,161,191 bigram tokens.\n",
    "\n",
    "Notice how many more bigrams **types** there are than unigram types (436,003 vs. 49,815).  Make sure you understand **why** that is.  Every time a word is followed by some word it's never been followed by, that's a new bigram type.  So we see above that the bigram 'the only' has occurred 258 times in Brown (that's quite high for a bigram.  But 'the' also occurs in all the following bigram types, each with a different count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251\n",
      "81\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "print(mle_bigram_dist['the','time'])\n",
    "print(mle_bigram_dist['the','boy'])\n",
    "print(mle_bigram_dist['the','red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 49, 815 word types in the vocabulary, there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2481534225\n",
      "2,481,534,225\n"
     ]
    }
   ],
   "source": [
    "print(49815**2)\n",
    "print(f'{49815**2:,}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "($49^2$) **possible bigrams types** for this vocabulary, but in the 1.2 million words of Brown, we see \n",
    "only 436,003 actual bigram types.That's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00017569896703721667\n",
      "0.018%\n"
     ]
    }
   ],
   "source": [
    "print(436003/(49815**2))\n",
    "print(f'{436003/(49815**2):.3%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".018 % of the possible bigrams, a very tiny fraction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
