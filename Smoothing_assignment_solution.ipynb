{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Pollard assignment you computed a unigram frequency distribution for the Brown corpus. You will need that for this assignmewnt.\n",
    "\n",
    "This time you will do a bigram distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk import bigrams\n",
    "brown_bigrams = list(bigrams(brown.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is instructive to compare brown.words, which we used in the last assignment, with brown.bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.words[:10]\n",
    "#['The', 'Fulton', 'County', 'Grand', 'Jury', 'said',\n",
    "# 'Friday', 'an', 'investigation', 'of']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_bigrams[:10]\n",
    "#[('The', 'Fulton'), ('Fulton', 'County'), ('County', 'Grand'),\n",
    "#('Grand', 'Jury'), ('Jury', 'said'), ('said', 'Friday'), ('Friday', 'an'),\n",
    "#('an', 'investigation'), ('investigation', 'of'), ('of', \"Atlanta's\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So brown.words() returns a list of the words, while brown.bigrams() returns a list of word pairs. Notice the the second word of the first pair becomes the first word of the second pair, and the the second word of the second pair, the first word of the third, and so on. Since each word in Brown becaome the first word of a bigram except the last, there is exactly one more word token than there are bogram tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(brown_bigrams)\n",
    "#1161191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(brown.words())\n",
    "1161192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new frequency distribution of the Brown bigrams. Plot the cumulative frequency distribution of the top 50 bigrams.\n",
    "\n",
    "Then do add one smoothing on the bigrams. This will require adding one to all the bigram counts, including those that previously had count 0. You will also need to change the ungram counts appropriately. You will compute all possible bigrams using the known vocabulary, so use the keys of the unigram Brown distribution you created before to compute the set of possible bigrams. The vocabulary size from that exercise should be 49815. Then having added 1 to all the bigram counts, you must compute at least the following Probabilities:\n",
    "\n",
    "\n",
    "1. P(the | in) before and after smoothing (P_{\\text{mle}} and P_{\\text{laplace}});\n",
    "\n",
    "2.  P(in the) before and after smoothing;\n",
    "\n",
    "3.  P(said the) before and after smoothing.\n",
    "\n",
    "4. P(the | said) before and after smoothing.\n",
    "\n",
    "In some cases you will to use the unigram counts to compute these probabilities. Remember that the unigram counts must change too when smoothing.\n",
    "\n",
    "Turn in these values and the Python code you used to compute them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "wds = brown.words()\n",
    "N = len(wds)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161192\n",
      "1161191\n"
     ]
    }
   ],
   "source": [
    "mle_unigram_dist = nltk.FreqDist([w.lower() for w in wds])\n",
    "bigram_seq = list(nltk.bigrams(wds))\n",
    "bigram_N = len(bigram_seq)\n",
    "print(bigram_N)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bigram_N` = `N - 1`.  Here's why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'Fulton'),\n",
       " ('Fulton', 'County'),\n",
       " ('County', 'Grand'),\n",
       " ('Grand', 'Jury'),\n",
       " ('Jury', 'said'),\n",
       " ('said', 'Friday'),\n",
       " ('Friday', 'an'),\n",
       " ('an', 'investigation'),\n",
       " ('investigation', 'of'),\n",
       " ('of', \"Atlanta's\")]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_seq[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first bigram starts with the first word, the second with second word and so on.  But there is no bigram\n",
    "that starts with the last word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a frequency distribution for bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE stands for Maximum Likelihood Estimate\n",
    "mle_bigram_dist = nltk.FreqDist((x.lower(),y.lower()) for (x,y) in bigram_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 49815 samples and 1161192 outcomes>\n",
      "69971\n",
      "<FreqDist with 436003 samples and 1161191 outcomes>\n",
      "258\n"
     ]
    }
   ],
   "source": [
    "print(mle_unigram_dist)\n",
    "print(mle_unigram_dist['the'])\n",
    "print(mle_bigram_dist)\n",
    "print(mle_bigram_dist['the','only'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information printed about `mle_unigram_dist`: The vocabulary has 49,815 word types.  The Brown corpus has 1,161,192 word tokens.\n",
    "\n",
    "The information printed about `mle_bigram_dist`: The \"vocabulary\" (of bigrams) has 436,003 bigram types.  The Brown corpus has 1,161,191 bigram tokens.\n",
    "\n",
    "Notice how many more bigrams **types** there are than unigram types (436,003 vs. 49,815).  Make sure you understand **why** that is.  Every time a word is followed by some word it's never been followed by, that's a new bigram type.  So we see above that the bigram 'the only' has occurred 258 times in Brown (that's quite high for a bigram.  But 'the' also occurs in all the following bigram types, each with a different count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251\n",
      "81\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "print(mle_bigram_dist['the','time'])\n",
    "print(mle_bigram_dist['the','boy'])\n",
    "print(mle_bigram_dist['the','red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 49, 815 word types in the vocabulary, there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2481534225\n",
      "2,481,534,225\n"
     ]
    }
   ],
   "source": [
    "print(49815**2)\n",
    "print(f'{49815**2:,}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "($49^2$) **possible bigrams types** for this vocabulary, but in the 1.2 million words of Brown, we see \n",
    "only 436,003 actual bigram types.That's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00017569896703721667\n",
      "0.018%\n"
     ]
    }
   ],
   "source": [
    "print(436003/(49815**2))\n",
    "print(f'{436003/(49815**2):.3%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".018 % of the possible bigrams, a very tiny fraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Probs\n",
    "\n",
    "Our events are bigrams.\n",
    "\n",
    "In general, we are solving the problem of predicting the **next** word knowing\n",
    "the **previous** words.  When we use a bigram model to do that, we are predicting the **next** word knowing\n",
    "only the **previous** word.  \n",
    "\n",
    "The notation \n",
    "\n",
    "```\n",
    "P(the | in)\n",
    "```\n",
    "\n",
    "means \n",
    "\n",
    "*the probability that \"the\" is the **second** word in a bigram  given that \"in\" is the **first**, so the bigram event we are looking for is \"in the\".\n",
    "\n",
    "This a **conditional probability**.  It is different from a joint probability.\n",
    "\n",
    "```\n",
    "P(A | B)  = P(A,B)/P(B)\n",
    "```\n",
    "\n",
    "The joint probability is on the right.\n",
    "\n",
    "What does an MLE (Maximum likelihood estimate) look like?\n",
    "\n",
    "$\n",
    "P_{mle}(A \\mid B) = \\frac{P(A,B)}{P(B)} = \\frac{\\mbox{count}(A,B)/N}{\\mbox{count}(B)/N} = \\frac{\\mbox{count}(A,B)}{\\mbox{count}(B)}\n",
    "$\n",
    "\n",
    "Notice the following **difference**:\n",
    "\n",
    "$\n",
    "P_{mle}(A \\mid B) = \\frac{\\mbox{count}(A,\\,B)}{\\mbox{count}(B)}\n",
    "$\n",
    "\n",
    "$\n",
    "P_{mle}(A,\\, B) = \\frac{\\mbox{count}(A,\\,B)}{N}\n",
    "$\n",
    "\n",
    "In terms of bigrams:\n",
    "\n",
    "$\n",
    "P_{mle}(the \\mid in) = \\frac{\\mbox{count}(in\\; the)}{\\mbox{count}(in)}\n",
    "$\n",
    "\n",
    "$\n",
    "P_{mle}(A,\\, B) = \\frac{\\mbox{count}(in\\; the)}{N}\n",
    "$\n",
    "\n",
    "That is, in the conditional probability, we are restricting our attention to a sample space\n",
    "in which the first word of a bigram event is \"in\", and the maximum likelihood probability\n",
    "is the proportion of those events that are \"in the\" events.\n",
    "\n",
    "That is, in the joint probability, we are looking at the entire sample space\n",
    "of bigram events, and the maximum likelihood probability\n",
    "is the proportion of all bigram events that are \"in the\" events.  Here N is the\n",
    "size of the corpus, the number total bigram events.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace Probs\n",
    "\n",
    "We add 1 to every possible bigram event (possible given the vocab we know).  We recompute \n",
    "probs using the same logic as befiore, but taking into account the new counts.\n",
    "\n",
    "Count of every bigram: goes up by 1\n",
    "\n",
    "Count of every unigram as a first word in a possible bigram:  goes up by V, the size of the\n",
    "vovacbulary, since it is the first word in V possible pbigrams, and the count of each of those\n",
    "has gone up by 1.\n",
    "\n",
    "N: the total number of bigram events has become $N + V^{2}$, because we added 1 to every possible \n",
    "bigram and are $V^{2}$ possible bigrams.\n",
    "\n",
    "\n",
    "For example, consider $P_{laplace}(the \\mid in)$\n",
    "\n",
    "In terms of bigrams:\n",
    "\n",
    "$\n",
    "P_{laplace}(the \\mid in) = \\frac{\\mbox{count}(in\\; the)\\, + \\,1}{\\mbox{count}(in)\\, +\\,V}\n",
    "$\n",
    "\n",
    "$\n",
    "P_{laplace}(A,\\, B) = \\frac{\\mbox{count}(in\\; the)\\,+\\,1}{N\\, +\\, V^{2}}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "wds = brown.words()\n",
    "N = len(wds)\n",
    "print(N)\n",
    "mle_unigram_dist = nltk.FreqDist([w.lower() for w in wds])\n",
    "\n",
    "bigram_seq = list(nltk.bigrams(wds))\n",
    "# MLE stands for Maximum Likelihood Estimate\n",
    "mle_bigram_dist = nltk.FreqDist((x.lower(),y.lower()) for (x,y) in bigram_seq)\n",
    "bigram_N = len(bigram_seq)\n",
    "\n",
    "ct_in_the = mle_bigram_dist[('in','the')]\n",
    "#6025\n",
    "\n",
    "p_the_given_in = float(mle_bigram_dist[('in','the')])/mle_unigram_dist['in']\n",
    "#0.2823733420818297\n",
    "\n",
    "\n",
    "p_in_the = mle_bigram_dist[('in','the')]/float(bigram_N)\n",
    "#0.013818712256567042\n",
    "\n",
    "ct_said_the = mle_bigram_dist[('said','the')]\n",
    "#74\n",
    "\n",
    "p_the_given_said = float(mle_bigram_dist[('said','the')])/float(mle_unigram_dist['said'])\n",
    "#0.03773584905660377\n",
    "\n",
    "p_said_the = float(mle_bigram_dist[('said','the')])/float(bigram_N)\n",
    "#0.00016972360281924666\n",
    "\n",
    "p_sewer_brother = 0.0\n",
    "\n",
    "mle_probs = [p_the_given_in,p_in_the,p_the_given_said,p_said_the,p_sewer_brother]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace count of \"sewer brother\": 1\n",
      "\n",
      "                    MLE      Smooth  \n",
      "the | in         0.2823733  0.0846919\n",
      "in the           0.0051886  0.0000024\n",
      "the | said       0.0377358  0.0014485\n",
      "said the         0.0000637  0.0000000\n",
      "sewer brother    0.0000000  0.0000000\n",
      "\n",
      "                      MLE         Smooth  \n",
      "in the           0.2823733421  0.0846919271\n",
      "said the         0.0051886382  0.0000024272\n",
      "sewer brother    0.0377358491  0.0014485476\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# vocab_size\n",
    "V = len(mle_unigram_dist)\n",
    "bigram_size = V**2\n",
    "sm_bigram_N = bigram_N + bigram_size\n",
    "\n",
    "#Fix bigram cts\n",
    "# A counter dictionary that returns 1 for unseen keys\n",
    "laplace_bigram_dist  = defaultdict(lambda: 1)\n",
    "for big in mle_bigram_dist:\n",
    "    laplace_bigram_dist[big] = mle_bigram_dist[big] + 1\n",
    "    \n",
    "# Any unigram of count 0 has smoothed count 1\n",
    "laplace_uni = defaultdict(lambda: 1)\n",
    "#Fix nonzero unigram cts to reflect what we just did to bigram counts\n",
    "for x in mle_unigram_dist:\n",
    "    laplace_uni[x] = mle_unigram_dist[x] +  V\n",
    "\n",
    "\n",
    "# An unseen bigram\n",
    "print ('Laplace count of \"sewer brother\": {0}'.format(laplace_bigram_dist[('sewer','brother')]))\n",
    "print()\n",
    "\n",
    "# Q: We added 1 event for every possible bigram.\n",
    "#    How much did the total size of the corpus go up by?\n",
    "\n",
    "\n",
    "sm_ct_in_the = laplace_bigram_dist[('in','the')]\n",
    "#6026\n",
    "\n",
    "sm_p_the_given_in = float(laplace_bigram_dist[('in','the')])/laplace_uni['in']\n",
    "# 0.08469192714189341\n",
    "\n",
    "sm_p_in_the = laplace_bigram_dist[('in','the')]/float(sm_bigram_N)\n",
    "# 0.0000024\n",
    "\n",
    "sm_ct_said_the = laplace_bigram_dist[('said','the')]\n",
    "# 75\n",
    "\n",
    "sm_p_the_given_said = float(laplace_bigram_dist[('said','the')])/laplace_uni['said']\n",
    "#0.0015056\n",
    "\n",
    "sm_p_said_the = float(laplace_bigram_dist[('said','the')])/float(sm_bigram_N)\n",
    "#3.020910237987889e-08\n",
    "\n",
    "sm_p_sewer_brother = float(laplace_bigram_dist[('sewer','brother')])/float(sm_bigram_N)\n",
    "\n",
    "laplace_probs = [sm_p_the_given_in,sm_p_in_the,sm_p_the_given_said,sm_p_said_the,sm_p_sewer_brother]\n",
    "\n",
    "\n",
    "print('{0:<16s} {1:^9s}  {2:^9s}'.format('','MLE','Smooth'))\n",
    "for (i,p) in enumerate(['the | in','in the','the | said','said the','sewer brother']):\n",
    "    print('{0:<16s} {1:6.7f}  {2:6.7f}'.format(p,mle_probs[i],laplace_probs[i]))\n",
    "\n",
    "print()\n",
    "print('{0:<16s} {1:^14s}  {2:^9s}'.format('','MLE','Smooth'))\n",
    "for (i,p) in enumerate(['in the','said the','sewer brother']):\n",
    "    print('{0:<16s} {1:6.10f}  {2:6.10f}'.format(p,mle_probs[i],laplace_probs[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalized bigram cts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just clarifying some stuff in the text.\n",
    "\n",
    "We \"smooth\" the counts in the smoothed model so that the number of events adds back up to N.\n",
    "\n",
    "Computing the smoothed  probs then uses exactly the same formula as the mle probs, but using smoothed counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  Raw cts   Smthd Cts\n",
      "in_the           6025.0  2.8179783\n",
      "said_the         74.0    0.0346109\n",
      "sewer_brother    0.0     0.0004677\n",
      "\n",
      "                  Raw cts   Smthd Cts\n",
      "in               21337   9.9796187\n",
      "\n",
      "P(the | in)  0.2823733420818296\n"
     ]
    }
   ],
   "source": [
    "\n",
    "norm_factor = float(N)/(N + V**2)\n",
    "\n",
    "bigram_norm_factor = float(bigram_N)/(bigram_N + V**2)\n",
    "\n",
    "new_cts = defaultdict(lambda:norm_factor)\n",
    "\n",
    "for w in mle_unigram_dist:\n",
    "    new_cts[w] = mle_unigram_dist[w] * norm_factor\n",
    "\n",
    "new_bigram_cts = defaultdict(lambda:bigram_norm_factor)\n",
    "\n",
    "for big in mle_bigram_dist:\n",
    "    new_bigram_cts[big] = mle_bigram_dist[big] * norm_factor\n",
    "\n",
    "new_laplace_cts = [float(new_bigram_cts[('in','the')]),\n",
    "                     float(new_bigram_cts[('said','the')]),\n",
    "                     float(new_bigram_cts[('sewer','brother')]),\n",
    "                     ]\n",
    "old_cts = [float(mle_bigram_dist[('in','the')]),\n",
    "                     float(mle_bigram_dist[('said','the')]),\n",
    "                     float(mle_bigram_dist[('sewer','brother')]),\n",
    "                     ]\n",
    "\n",
    "\n",
    "print()\n",
    "print('{0:<16s} {1:^9s}  {2:^9s}'.format('','Raw cts','Smthd Cts'))\n",
    "for (i,p) in enumerate(['in_the','said_the','sewer_brother']):\n",
    "    print('{0:<16s} {1:<6}  {2:<6.7f}'.format(p,old_cts[i],new_laplace_cts[i]))\n",
    "    \n",
    "    \n",
    "print()\n",
    "print('{0:<16s} {1:^9s}  {2:^9s}'.format('','Raw cts','Smthd Cts'))\n",
    "for (i,p) in enumerate(['in']):\n",
    "    print('{0:<16s} {1:<6}  {2:<6.7f}'.format(p,mle_unigram_dist[p],new_cts[p]))\n",
    "\n",
    " \n",
    "print()\n",
    "print('P(the | in)', end = '  ')\n",
    "print(new_bigram_cts['in','the']/new_cts['in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
