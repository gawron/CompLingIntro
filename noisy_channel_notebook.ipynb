{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "noisy_channel_notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t546DHtd9SYE"
      },
      "source": [
        "# Noisy channel spelling correction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmSG_ra29SYJ"
      },
      "source": [
        "## Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1_WhU-x9SYJ"
      },
      "source": [
        "Norvig resources\n",
        "\n",
        "1. [Spell checking blog post](http://norvig.com/spell-correct.html)\n",
        "2. [count_1edit.txt: error counts file for 1 edit errors](https://norvig.com/ngrams/count_1edit.txt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwU_PpvB9SYJ"
      },
      "source": [
        "A couple of comments on the resources above.  \n",
        "\n",
        "Norvig's Python spelling correcter is beautiful python code (like his [Sudoku solver](https://norvig.com/sudoku.html)), but it isn't actually a noisy channel model, because it doesn't use a probabilistic error model, although he tries to talk into you into thinking that it does.  However, it's designed so that you could turn it into a real noisy channel model quite easily, and you might try looking at the code and thinking about how to do that (but read the blog post first, so as to have a take on how to approach the code).\n",
        "An extract from that code is used in this assignment.\n",
        "\n",
        "The file `count_1edit.txt` is based obn data about misspellings (see Norvig's post) and contains counts of errors that are 1 edit away from the correct word. We use those counts for our error model.  We use `spell-errors.txt`, the source file from which `count_1edit.txt` was derived, to get the total unigraph and bigraph counts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEQMkuff9SYJ"
      },
      "source": [
        "## Setup code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55-SATgM9SYK"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "import codecs\n",
        "import re\n",
        "import urllib\n",
        "\n",
        "def open_file_or_url(path):\n",
        "    if re.match(r'https?://', path):\n",
        "        return urllib.request.urlopen(path)\n",
        "    else:\n",
        "        # Paralleling what urlopen does, we are going to open a bytestream (not a decoded string stream)\n",
        "        # and read and decode it separately.\n",
        "        return open(path, 'rb')\n",
        " \n",
        "def get_letter_counts (path, encoding='latin1', linesep = r'\\n', errors='strict'):\n",
        "    \"\"\"\n",
        "    Build a single nltk.FreqDist instance with counts of both unigraphs\n",
        "    and bigraphs for the words in `data_file`.\n",
        "    \"\"\"\n",
        "    global line\n",
        "    letters = []\n",
        "    with open_file_or_url(path) as handle:\n",
        "        bytesdata = handle.read()\n",
        "        data = codecs.decode(bytesdata,encoding=encoding,errors=errors)\n",
        "    for (lct,line) in enumerate(re.split(linesep,data)):\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "           #print(line)\n",
        "           (right,wrong) = line.split(\":\")\n",
        "           letters.append(right)\n",
        "           letters.extend(ww.strip()  for ww in wrong.split(\",\"))\n",
        "    # One big long string of letters with spaces separating words\n",
        "    letters = ' '.join(letters)\n",
        "    fd = nltk.FreqDist(letters)\n",
        "    blts = list(nltk.bigrams(letters))\n",
        "    fd.update(blts)\n",
        "    return (fd, letters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYsXk9R2-ait"
      },
      "source": [
        "You will need to execute the following download code each time you run the notebook if you are running in google colab.  If you run this on your local machine, that will install the `brown` corpus permanently on your machine and you will only have to run this cell once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZyf0ETq-Jpd",
        "outputId": "4bb21c63-dae9-4a6f-ad47-a8b094f4d932"
      },
      "source": [
        "nltk.download('brown')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w259m-G9SYK"
      },
      "source": [
        "Get the counts necessary for computing bigram model for the lower-cased Brown corpus.  This takes a littke while."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sga_E6ch9SYK"
      },
      "source": [
        "from nltk.corpus import brown\n",
        "brown_lc_words = [wd.lower() for wd in brown.words()]\n",
        "WORDS = set(brown_lc_words)\n",
        "ugr_words =  nltk.FreqDist(brown_lc_words)\n",
        "bigr_words =  nltk.FreqDist(nltk.bigrams(brown_lc_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nicnR8Gs9SYL"
      },
      "source": [
        "big_voc = list(bigr_words.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiGVf7Hi9SYL",
        "outputId": "6c2c4d50-0c0c-4c8a-a867-e9d4eb33034c"
      },
      "source": [
        "[(w1,w2) for (w1,w2) in big_voc if w2 == 'francisco']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('san', 'francisco'), (',', 'francisco'), ('of', 'francisco')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nmhByI19SYM",
        "outputId": "f19d89b6-a3ad-4e9c-b76f-1f5e030f764c"
      },
      "source": [
        "ugr_words['to']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26158"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gpletd3p9SYM"
      },
      "source": [
        "Get the data from the mispelling lists Norvig used to compile error counts.\n",
        "\n",
        "Compute the necessary **unigraph** and **bigraph** counts for what's coming.  We'll call these kinds of information our **ngraph model**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz1TJlOq9SYM"
      },
      "source": [
        "url = 'https://norvig.com/ngrams/spell-errors.txt'\n",
        "letter_freqs, letters = get_letter_counts (url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWrychiD9SYN"
      },
      "source": [
        "The `letterFreqs` dictionary contains both unigram letter frequencies and bigram letter frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jUHb-Vy9SYN",
        "outputId": "c7cc99d7-74c5-472a-c7bc-0f37aaf27f2b"
      },
      "source": [
        "letter_freqs['i']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29799"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSWfFGjP9SYN",
        "outputId": "2d6219f6-ac34-4527-c4d9-b61239809d00"
      },
      "source": [
        "letter_freqs['o','r']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2857"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkuW-W469SYN"
      },
      "source": [
        "## Load Norvig's error data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2l17PFM9SYO"
      },
      "source": [
        " \n",
        "def make_err_dict (path, encoding='latin1',errors='strict',linesep = r'\\n'):\n",
        "    \"\"\"\n",
        "    To get past some funky unicode characters, do errors = 'ignore'\n",
        "    \n",
        "    This will lose the troublesome characters; it  will not perform a reasonable\n",
        "    approximation, such as \"e\"  for \"e with an accent mark\". \n",
        "    \"\"\"\n",
        "    global error_cts\n",
        "    error_cts = dict()\n",
        "    with open_file_or_url(path) as handle:\n",
        "        bytesdata = handle.read()\n",
        "        data = codecs.decode(bytesdata,encoding=encoding,errors=errors)\n",
        "    for (lct,line) in enumerate(re.split(linesep,data)):\n",
        "        process_line (line, error_cts, lct)\n",
        "    return error_cts\n",
        "\n",
        "def process_line (line, error_cts,lct):\n",
        "     line = line.strip()\n",
        "     if line:\n",
        "        (err, ct) = line.split('\\t')\n",
        "        (tgt, src) = err.split('|')\n",
        "        if tgt and src:\n",
        "           error_cts[tgt,src] = int(ct)\n",
        "        elif tgt:\n",
        "            error_cts[tgt,'#'] = int(ct)\n",
        "        elif src:\n",
        "            error_cts['#', src] = int(ct)\n",
        "        else:\n",
        "            print(f'***Warning*** Bad line {lct}: {line}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAVXF8v09SYO"
      },
      "source": [
        "The `make_err_dict` function loads Norvig's data on error counts and makes a dictionary of it.  We'll call that our **error counts data**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gLFRW5F9SYO",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd2c1ff3-a3e1-4574-98bd-87c887098612"
      },
      "source": [
        "# Error data on Peter Norvig's website\n",
        "url = 'https://norvig.com/ngrams/count_1edit.txt'\n",
        "err_dict = make_err_dict(url)\n",
        "# OR Downloaded as a file on your local machine\n",
        "#fn = 'count_1edit.txt'\n",
        "#err_dict = make_err_dict(fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***Warning*** Bad line 415: |\t19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-Q0dMcD9SYO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a592115-b27a-4e1d-90f4-702854b11294"
      },
      "source": [
        "#count('e'|'i'): Sub e for i\n",
        "# This shd return 917\n",
        "err_dict['e','i']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "917"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt1ORvVJ9SYO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30b80cdc-185f-4c6d-f75f-26452a6e3ef3"
      },
      "source": [
        "#count('es'|'e'): Insert s after e\n",
        "#This should return 136\n",
        "err_dict['es','e']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "136"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0nmcV4s9SYP"
      },
      "source": [
        "Some errors are not that frequent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTxTVxdK9SYP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43672482-3964-45f2-a728-80dc9e835dbc"
      },
      "source": [
        "err_dict['eh','he']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZD2fHHP9SYP"
      },
      "source": [
        "Some errors we simply have no data for:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUh-cIdS9SYQ",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "d71c2279-8a8c-4c51-cd83-6543815c137e"
      },
      "source": [
        "err_dict['he','eh']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-96e4b408fe21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0merr_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'he'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'eh'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: ('he', 'eh')"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzwirgA59SYQ"
      },
      "source": [
        "To compute an error probability, we need the ngram model (`ugr_letter_freqs` and `bigr_letter_freqs`) and the error counts data (`err_dict`), both loaded in cells above this one.\n",
        "\n",
        "Using the probability formula for an insertion on Slide 24, we get:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYcHAm2R9SYQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d43894-8e28-4e73-80f4-ae74ffe38e7d"
      },
      "source": [
        "#P(es|e)\n",
        "err_dict[\"es\",\"e\"]/letter_freqs['e']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.002943022224145766"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ-TIPgC9SYQ"
      },
      "source": [
        "From Peter Norvig's `spell.py`, the function `candidates` defined in the next cell is an amended version of Norvig's Python function.  It generates candidate corrections one edit distance away from a misspelling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h25emKv49SYQ"
      },
      "source": [
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or  [word])\n",
        "\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All 'words' that are one edit operation away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFZyehSh9SYR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "225a1636-27cf-4f81-f034-a8b3d5497cfd"
      },
      "source": [
        "candidates('potch')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'notch', 'patch', 'pitch', 'poach', 'porch', 'pouch'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeUQSrBT9SYR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9360eb4-9ca9-4ac2-95e3-13dcaf32fef0"
      },
      "source": [
        "candidates('pooch')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cooch', 'hooch', 'poach', 'porch', 'pouch'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDWkKPgl9SYR"
      },
      "source": [
        "Our model doesn't know the word \"pooch\" (it just didn't show up in Brown).  If you give `candidates` a word that **is** in Brown, it only returns one candidate:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLv1qyt49SYR"
      },
      "source": [
        "As a result, the spelling correcter we're building won't handle mispellings that are real words (\"thew\" for \"the\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuIgJmRS9SYS"
      },
      "source": [
        "## The assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSAcWcPi9SYS"
      },
      "source": [
        "Here is a sentence with a mispelling.  All punctuation has been removed because your error model doesn't include\n",
        "punctuation errors.\n",
        "\n",
        "```\n",
        "I found John on the back potch of his house stroking a big brown dog\n",
        "```\n",
        "\n",
        "1.  Use `err_dict`, and `letter_freqs` to compute your error probabilities (P(x | w))\n",
        "    using the formulas on slide 24 in the Noisy Channel spelling correction slides.  You can safely assume any misspelling in the sentence above is 1 edit\n",
        "    distance from the intended word, and that's what the candidate-generating code below does.\n",
        "\n",
        "2.  Find the candidate correct words using Norvig's `candidates` function defined below.  For instance, given the mispelling \"thorw\", `candidates` would find\n",
        "\n",
        "    ```\n",
        "    >>> candidates('thorw')\n",
        "    {'thor', 'thorn', 'thorp', 'throw'}\n",
        "    ```\n",
        "\n",
        "    The inclusion of 'thor' and 'thorp' is probably because they are proper names and we have lower-cased the corpus.     Don't worry about this.  For this assignment, just consider every candidate `candidates` returns.\n",
        "\n",
        "3.  For our language model we will compute the probability of each candidate  word w in context:\n",
        "\n",
        "    $\\mbox{p}_{con}(\\mbox{w})$.  \n",
        "    \n",
        "    To compute that, use the bigram language model defined by the lower-cased Brown corpus we loaded above to compute \n",
        "    \n",
        "    $\\mbox{p}_{con}(\\mbox{w})$ \n",
        "    \n",
        "    (the probability of candidate word w in context). You do not     have to consider only the previous word when you use a bigram model.  You can take both the preceding word  prev and the following word foll into account by computing the probability of  the 3-word sequence consisting of prev, w, foll; so  \n",
        "\n",
        "    $\\mbox{p}_{con}(\\mbox{w})$, \n",
        "\n",
        "    the provbability of candidate word w in the context of words prev and foll, is:\n",
        "    \n",
        "    $\\mbox{p}_{con}(\\mbox{w}) = \\mbox{p}(\\mbox{prev}) * \\mbox{p}(\\mbox{w} \\mid \\mbox{prev}) * \\mbox{p}(\\mbox{foll} \\mid \\mbox{c})$\n",
        "    \n",
        "    So for example if our given sentence included the three word sequence \"boys thorw the\" and we wanted to \n",
        "    consider to correct it to \"boys throw the\", the candidate word would be *throw* and we would have: \n",
        "    \n",
        "    $$\\mbox{p}_{con}(\\mbox{throw}) = \\mbox{p}(\\mbox{boys}) * \\mbox{p}(\\mbox{throw} \\mid \\mbox{boys}) * \n",
        "       \\mbox{p}(\\mbox{the} \\mid \\mbox{throw})$$\n",
        "    \n",
        "    This uses a unigram model for $\\mbox{p}(\\mbox{boys})$  and a bigram model for $\\mbox{p}(\\mbox{throw} \\mid \\mbox{boys})$ and  $\\mbox{p}(\\mbox{the} \\mid \\mbox{throw})$. You should use the counts provided above for the Brown data earelier in this notebook to compute unsmoothed MLE probabilities for the unigrams and the bigrams.\n",
        "\n",
        "4.  For each candidate word w, use the noisy channel model to score all the candidates. $$\\mbox{p}(x \\mid \\mbox{w}) * p_{con}(\\mbox{w})$$\n",
        "\n",
        "5.  State what the best correction the model has chosen is.\n",
        "\n",
        "6.  To turn in.  An edited version of this Python notebook that you will save (using the pull down menu \"File\" and choosing \"Download .ipynb\") and rename to\n",
        "\n",
        "    ```\n",
        "    [first_name]_[last_name]_noisy_channel_notebook.ipynb\n",
        "    ```\n",
        "\n",
        "    for example \"jane_smith_noisy_channel_notebook.ipynb\".  You can create new cells by clicking on any existing cell and hovering over the center of the lower edge or the upper edge (either will work) until small +Code or +Text tabs appear. Click on whichever kind of new cell you want.  Use the upper edge if you want your new cell to appear **before** the existing cell; use the lower edge for a new cell **after** the existing cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTqOif1g9SYS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}