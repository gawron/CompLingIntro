{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy channel spelling correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Norvig resources\n",
    "\n",
    "1. [Spell checking blog post](http://norvig.com/spell-correct.html)\n",
    "2. [count_1edit.txt: error counts file for 1 edit errors](https://norvig.com/ngrams/count_1edit.txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of comments on the resources above.  \n",
    "\n",
    "Norvig's Python spelling correcter is beautiful python code (like his [Sudoku solver](https://norvig.com/sudoku.html)), but it isn't actually a noisy channel model, because it doesn't use a probabilistic error model, although he tries to talk into you into thinking that it does.  However, it's designed so that you could turn it into a real noisy channel model quite easily, and you might try looking at the code and thinking about how to do that (but read the blog post first, so as to have a take on how to approach the code).\n",
    "An extract from that code is used in this assignment.\n",
    "\n",
    "The file `count_1edit.txt` is based obn data about misspellings (see Norvig's post) and contains counts of errors that are 1 edit away from the correct word. We use those counts for our error model.  We use `spell-errors.txt`, the source file from which `count_1edit.txt` was derived, to get the total unigraph and bigraph counts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import codecs\n",
    "\n",
    "def get_letter_counts (data_file ='spell-errors.txt', encoding='latin1'):\n",
    "    \"\"\"\n",
    "    Build a single nltk.FreqDist instance with counts of both unigraphs\n",
    "    and bigraphs for the words in `data_file`.\n",
    "    \"\"\"\n",
    "    global line\n",
    "    letters = []\n",
    "    with codecs.open(data_file, 'r', encoding = encoding) as fh:\n",
    "        for (lct,line) in enumerate(fh):\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                #print(line)\n",
    "                (right,wrong) = line.split(\":\")\n",
    "                letters.append(right)\n",
    "                letters.extend(ww.strip()  for ww in wrong.split(\",\"))\n",
    "    # One big long string of letters with spaces separating words\n",
    "    letters = ' '.join(letters)\n",
    "    fd = nltk.FreqDist(letters)\n",
    "    blts = list(nltk.bigrams(letters))\n",
    "    fd.update(blts)\n",
    "    return (fd, letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the counts necessary for computing bigram model for the lower-cased Brown corpus.  This takes a littke while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_lc_words = [wd.lower() for wd in brown.words()]\n",
    "WORDS = set(brown_lc_words)\n",
    "ugr_words =  nltk.FreqDist(brown_lc_words)\n",
    "bigr_words =  nltk.FreqDist(nltk.bigrams(brown_lc_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_voc = list(bigr_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('san', 'francisco'), (',', 'francisco'), ('of', 'francisco')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(w1,w2) for (w1,w2) in big_voc if w2 == 'francisco']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26158"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ugr_words['to']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data from the mispelling lists Norvig used to compile error counts.\n",
    "\n",
    "Compute the necessary **unigraph** and **bigraph** counts for what's coming.  We'll call these kinds of information our **ngraph model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_freqs, letters = get_letter_counts ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({' ': 47550, 'e': 46211, 'i': 29799, 'a': 29298, 'n': 27727, 't': 26161, 's': 24828, 'r': 24599, 'o': 21493, 'l': 18625, ...})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `letterFreqs` dictionary contains both unigram letter frequencies and bigram letter frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29799"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_freqs['i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2857"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_freqs['o','r']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Norvig's error data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the function defined in the next cell requires that there be a copy of the file `count_1edit.txt` in the same directory as this notebook. I have tried to facilitate this by supplying you with a zip file that includes the error data file.  Reading the file in to Python only worked for me if I specified the Python `'latin1'` codec, \n",
    "but once I did that, I  was able to use the identical file supplied on\n",
    "[Norvig's NLP code and data page](https://norvig.com/ngrams/).  \n",
    "\n",
    "If you have trouble finding the error file on your computer, just edit the cell that calls `make_err_dict` to\n",
    "use a full path name:\n",
    "\n",
    "```\n",
    "make_error_dict(<full_path_string_to count_1edit.txt>)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import urllib\n",
    "import re\n",
    "\n",
    "def open_file_or_url(path):\n",
    "    if re.match(r'https?://', path):\n",
    "        return urllib.request.urlopen(path)\n",
    "    else:\n",
    "        # Paralleling what urlopen does, we are going to open a bytestream (not a decoded string stream)\n",
    "        # and read and decode it separately.\n",
    "        return open(path, 'rb')\n",
    "    \n",
    "def make_err_dict (path, encoding='latin1',errors='strict',linesep = r'\\n'):\n",
    "    \"\"\"\n",
    "    To get past some funky unicode characters, do errors = 'ignore'\n",
    "    \n",
    "    This will lose the troublesome characters; it  will not perform a reasonable\n",
    "    approximation, such as \"e\"  for \"e with an accent mark\". \n",
    "    \"\"\"\n",
    "    global error_cts\n",
    "    error_cts = dict()\n",
    "    with open_file_or_url(path) as handle:\n",
    "        bytesdata = handle.read()\n",
    "        data = codecs.decode(bytesdata,encoding=encoding,errors=errors)\n",
    "    for (lct,line) in enumerate(re.split(linesep,data)):\n",
    "        process_line (line, error_cts, lct)\n",
    "    return error_cts\n",
    "\n",
    "def process_line (line, error_cts,lct):\n",
    "     line = line.strip()\n",
    "     if line:\n",
    "        (err, ct) = line.split('\\t')\n",
    "        (tgt, src) = err.split('|')\n",
    "        if tgt and src:\n",
    "           error_cts[tgt,src] = int(ct)\n",
    "        elif tgt:\n",
    "            error_cts[tgt,'#'] = int(ct)\n",
    "        elif src:\n",
    "            error_cts['#', src] = int(ct)\n",
    "        else:\n",
    "            print(f'***Warning*** Bad line {lct}: {line}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `make_err_dict` function loads Norvig's data on error counts and makes a dictionary of it.  We'll call that our **error counts data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Warning*** Bad line 415: |\t19\n"
     ]
    }
   ],
   "source": [
    "# Error data on Peter Norvig's website\n",
    "url = 'https://norvig.com/ngrams/count_1edit.txt'\n",
    "err_dict = make_err_dict(url)\n",
    "# OR Downloaded as a file on your local machine\n",
    "#fn = 'count_1edit.txt'\n",
    "#err_dict = make_err_dict(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "917"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count('e'|'i'): Sub e for i\n",
    "# This shd return 917\n",
    "err_dict['e','i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count('es'|'e'): Insert s after e\n",
    "#This should return 136\n",
    "err_dict['es','e']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some errors are not that frequent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_dict['eh','he']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some errors we simply have no data for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('he', 'eh')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-96e4b408fe21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0merr_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'he'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'eh'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: ('he', 'eh')"
     ]
    }
   ],
   "source": [
    "err_dict['he','eh']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute an error probability, we need the ngram model (`ugr_letter_freqs` and `bigr_letter_freqs`) and the error counts data (`err_dict`), both loaded in cells above this one.\n",
    "\n",
    "Using the probability formula for an insertion on Slide 24, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002943022224145766"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#P(es|e)\n",
    "err_dict[\"es\",\"e\"]/letter_freqs['e']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Peter Norvig's `spell.py`, the function `candidates` defined in the next cell is an amended version of Norvig's Python function.  It generates candidate corrections one edit distance away from a misspelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or  [word])\n",
    "\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All 'words' that are one edit operation away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'notch', 'patch', 'pitch', 'poach', 'porch', 'pouch'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates('potch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cooch', 'hooch', 'poach', 'porch', 'pouch'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates('pooch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model doesn't know the word \"pooch\" (it just didn't show up in Brown).  If you give `candidates` a word that **is** in Brown, it only returns one candidate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, the spelling correcter we're building won't handle mispellings that are real words (\"thew\" for \"the\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sentence with a mispelling.  All punctuation has been removed because your error model doesn't include\n",
    "punctuation errors.\n",
    "\n",
    "```\n",
    "I found John on the back potch of his house stroking a big brown dog\n",
    "```\n",
    "\n",
    "1.  Use `err_dict`, and `letter_freqs` to compute your error probabilities (P(x | w))\n",
    "    using the formulas on the slides.  You can safely assume any misspelling in the sentence above is 1 edit\n",
    "    distance from the intended word, and that's what the candidate-generating code below does.\n",
    "\n",
    "2.  Find the candidate correct words using Norvig's `candidates` function defined below.  For instance,\n",
    "    given the mispelling \"thorw\", `candidates` would find\n",
    "```\n",
    ">>> candidates('thorw')\n",
    "{'thor', 'thorn', 'thorp', 'throw'}\n",
    "```\n",
    "    The inclusion of 'thor' and 'thorp' is probably because they are proper names and we have lower-cased the corpus.     Don't worry about this.  For this assignment, just consider every candidate `candidates` returns.\n",
    "\n",
    "3.  For our language model we will compute the probability of each candidate  word w in context:\n",
    "    $\\mbox{P}_{con}(\\mbox{w})$.  To compute that, use the bigram language model defined by the lower-cased \n",
    "    Brown corpus we loaded above to compute $\\mbox{p}_{con}(\\mbox{w})$ (the probability of candidate word w in context). You do not     have to consider only the previous word when you use a bigram model.  You can take both the preceding word $\\mbox{prev}$\n",
    "    and the following word $\\mbox{foll}$ into account by computing the probability of \n",
    "    the 3-word sequence consisting of prev, w, foll; so  $mbox{p}_{con}(\\mbox{w})$, the provbability\n",
    "    of candidate word w in the context of words prev and foll, is:\n",
    "    \n",
    "    $$\n",
    "    \\mbox{p}_{con}(\\mbox{w}) = p(\\mbox{prev}) * p(\\mbox{w} \\mid \\mbox{prev}) * P(\\mbox{foll} \\mid \\mbox{c})\n",
    "    $$\n",
    "    \n",
    "    So for example if our given sentence included the three word sequence \"boys thorw the\" and we wanted to \n",
    "    consider to correct it to \"boys throw the\", the candidate word would be throw and we would have:\n",
    "\n",
    "    $$\n",
    "    \\mbox{p}_{con}(\\mbox{throw}) = \\mbox{p}(\\mbox{boys}) * \\mbox{p}(\\mbox{throw} \\mid \\mbox{boys}) * \n",
    "       \\mbox{p}(\\mbox{the} \\mid \\mbox{throw})\n",
    "    $$\n",
    "    \n",
    "    You may use an unsmoothed MLE model as your bigram model.\n",
    "\n",
    "4.  For each candidate word w, use the noisy channel model to score all the candidates.\n",
    "\n",
    "$$\n",
    "p(x \\mid \\mbox{w}) * p_{con}(\\mbox{w})\n",
    "$$\n",
    "\n",
    "5.  State what the best correction the model has chosen is.\n",
    "6.  To turn in.  An edited version of this Python notebook that you will save (using the pull down menu \"File\"\n",
    "    and choosing \"Save and checkpoint\") and rename to \"[your_first_name]_[your_last_name]_noisy_channel_nitebook.ipynb\", for\n",
    "    example \"joe_smith_spell.ipynb\" (using the \"File\" menu and choosing \"Rename\").  You can create new cells\n",
    "    by investigating the \"Help\" Menu and looking at \"Keyboard Shortcuts\".  Look for \"Change cell to markdown\" to \n",
    "    cells contain text, like this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
